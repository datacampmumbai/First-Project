{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Question No: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"Around a dozen bags are up for sale on the website eBay, with asking prices of up to £1,000.The bags were given to the 1,200 members of the public chosen by ballot to follow proceedings from the grounds of Windsor Castle. The bags contain items such as orders of service, fridge magnets, ponchos, shortbread and chocolate coins. One online listing says it offers the chance to buy your very own piece of British royal history Friday's wedding was also attended by 850 private guests, who were not given gift bags. Among those 850 were celebritiessuch as model Cara Delevingne and singer Robbie Williams. Gift bags were also given to the 2,640 members of the public invited into the grounds of Windsor Castle for the Duke and Duchess of Sussex's wedding in May. A number of those gift bags were sold online, with many auctions fetching more than £1,000.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Around a dozen bags are up for sale on the website eBay, with asking prices of up to £1,000.The bags were given to the 1,200 members of the public chosen by ballot to follow proceedings from the grounds of Windsor Castle.', 'The bags contain items such as orders of service, fridge magnets, ponchos, shortbread and chocolate coins.', \"One online listing says it offers the chance to buy your very own piece of British royal history Friday's wedding was also attended by 850 private guests, who were not given gift bags.\", 'Among those 850 were celebritiessuch as model Cara Delevingne and singer Robbie Williams.', \"Gift bags were also given to the 2,640 members of the public invited into the grounds of Windsor Castle for the Duke and Duchess of Sussex's wedding in May.\", 'A number of those gift bags were sold online, with many auctions fetching more than £1,000.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "psm = PorterStemmer()\n",
    "st = LancasterStemmer()\n",
    "lmtzr=WordNetLemmatizer()\n",
    "orignal_sentences = sent_tokenize(Text)\n",
    "print(orignal_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['around', 'dozen', 'bag', 'sale', 'websit', 'ebay', ',', 'ask', 'price', '£1,000.the', 'bag', 'given', '1,200', 'member', 'public', 'chosen', 'ballot', 'follow', 'proceed', 'ground', 'windsor', 'castl', '.', 'bag', 'contain', 'item', 'order', 'servic', ',', 'fridg', 'magnet', ',', 'poncho', ',', 'shortbread', 'chocol', 'coin', '.', 'one', 'onlin', 'list', 'say', 'offer', 'chanc', 'buy', 'piec', 'british', 'royal', 'histori', 'friday', \"'s\", 'wed', 'also', 'attend', '850', 'privat', 'guest', ',', 'given', 'gift', 'bag', '.', 'among', '850', 'celebritiessuch', 'model', 'cara', 'delevingn', 'singer', 'robbi', 'william', '.', 'gift', 'bag', 'also', 'given', '2,640', 'member', 'public', 'invit', 'ground', 'windsor', 'castl', 'duke', 'duchess', 'sussex', \"'s\", 'wed', 'may', '.', 'number', 'gift', 'bag', 'sold', 'onlin', ',', 'mani', 'auction', 'fetch', '£1,000', '.']\n"
     ]
    }
   ],
   "source": [
    "paras_stemmedSansStopWords = [psm.stem(word.lower()) for word in word_tokenize(Text) if word.lower() not in stopwords.words('english')]\n",
    "print(paras_stemmedSansStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_para = \" \".join([word for word in paras_stemmedSansStopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"around dozen bag sale websit ebay , ask price £1,000.the bag given 1,200 member public chosen ballot follow proceed ground windsor castl . bag contain item order servic , fridg magnet , poncho , shortbread chocol coin . one onlin list say offer chanc buy piec british royal histori friday 's wed also attend 850 privat guest , given gift bag . among 850 celebritiessuch model cara delevingn singer robbi william . gift bag also given 2,640 member public invit ground windsor castl duke duchess sussex 's wed may . number gift bag sold onlin , mani auction fetch £1,000 .\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['around', 'dozen', 'bag', 'sale', 'websit', 'ebay', 'ask', 'price', '£1,000.the', 'bag', 'given', '1,200', 'member', 'public', 'chosen', 'ballot', 'follow', 'proceed', 'ground', 'windsor', 'castl', 'bag', 'contain', 'item', 'order', 'servic', 'fridg', 'magnet', 'poncho', 'shortbread', 'chocol', 'coin', 'one', 'onlin', 'list', 'say', 'offer', 'chanc', 'buy', 'piec', 'british', 'royal', 'histori', 'friday', \"'s\", 'wed', 'also', 'attend', '850', 'privat', 'guest', 'given', 'gift', 'bag', 'among', '850', 'celebritiessuch', 'model', 'cara', 'delevingn', 'singer', 'robbi', 'william', 'gift', 'bag', 'also', 'given', '2,640', 'member', 'public', 'invit', 'ground', 'windsor', 'castl', 'duke', 'duchess', 'sussex', \"'s\", 'wed', 'may', 'number', 'gift', 'bag', 'sold', 'onlin', 'mani', 'auction', 'fetch', '£1,000']\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "custom = set(stopwords.words('english')+list(punctuation) + [\"'\",'\"',\"“\",'’'])\n",
    "stemmed_words = [word for word in paras_stemmedSansStopWords if word not in custom]\n",
    "print(stemmed_words)\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'bag': 6, 'given': 3, 'gift': 3, 'member': 2, 'public': 2, 'ground': 2, 'windsor': 2, 'castl': 2, 'onlin': 2, \"'s\": 2, 'wed': 2, 'also': 2, '850': 2, 'around': 1, 'dozen': 1, 'sale': 1, 'websit': 1, 'ebay': 1, 'ask': 1, 'price': 1, '£1,000.the': 1, '1,200': 1, 'chosen': 1, 'ballot': 1, 'follow': 1, 'proceed': 1, 'contain': 1, 'item': 1, 'order': 1, 'servic': 1, 'fridg': 1, 'magnet': 1, 'poncho': 1, 'shortbread': 1, 'chocol': 1, 'coin': 1, 'one': 1, 'list': 1, 'say': 1, 'offer': 1, 'chanc': 1, 'buy': 1, 'piec': 1, 'british': 1, 'royal': 1, 'histori': 1, 'friday': 1, 'attend': 1, 'privat': 1, 'guest': 1, 'among': 1, 'celebritiessuch': 1, 'model': 1, 'cara': 1, 'delevingn': 1, 'singer': 1, 'robbi': 1, 'william': 1, '2,640': 1, 'invit': 1, 'duke': 1, 'duchess': 1, 'sussex': 1, 'may': 1, 'number': 1, 'sold': 1, 'mani': 1, 'auction': 1, 'fetch': 1, '£1,000': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(stemmed_words)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(sent):\n",
    "    words = word_tokenize(sent)\n",
    "    score = [counts[x] for x in words]\n",
    "    return sum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 16, 36, 10, 34, 17]\n"
     ]
    }
   ],
   "source": [
    "scores = [score(sent) for sent in sent_tokenize(stemmed_para)]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around a dozen bags are up for sale on the website eBay, with asking prices of up to £1,000.The bags were given to the 1,200 members of the public chosen by ballot to follow proceedings from the grounds of Windsor Castle.\n",
      "One online listing says it offers the chance to buy your very own piece of British royal history Friday's wedding was also attended by 850 private guests, who were not given gift bags.\n"
     ]
    }
   ],
   "source": [
    "threshold = sorted(scores)[-3]\n",
    "for i,x in enumerate(scores):\n",
    "    if x > threshold:\n",
    "        print(orignal_sentences[i])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question No: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Windsor', 'NNP')\n",
      "('Castle', 'NNP')\n",
      "('Friday', 'NNP')\n",
      "('Cara', 'NNP')\n",
      "('Delevingne', 'NNP')\n",
      "('Robbie', 'NNP')\n",
      "('Williams', 'NNP')\n",
      "('Windsor', 'NNP')\n",
      "('Castle', 'NNP')\n",
      "('Duke', 'NNP')\n",
      "('Duchess', 'NNP')\n",
      "('Sussex', 'NNP')\n",
      "('May', 'NNP')\n",
      "[('eBay', 'ORGANIZATION'), ('Windsor Castle', 'ORGANIZATION'), ('British', 'GPE'), ('Cara Delevingne', 'PERSON'), ('Robbie Williams', 'PERSON'), ('Windsor Castle', 'ORGANIZATION'), ('Duke', 'ORGANIZATION'), ('Duchess', 'ORGANIZATION'), ('Sussex', 'GPE')]\n",
      "{'British', 'Sussex'}\n"
     ]
    }
   ],
   "source": [
    "###Question#2\n",
    "import nltk\n",
    "from textblob import TextBlob as tb\n",
    "blob=tb(Text)\n",
    "#print(blob)\n",
    "a=blob.tags\n",
    "#print(blob.tags)\n",
    "for i,x in enumerate(a):\n",
    "    if a[i][1]=='NNP':\n",
    "        print(a[i])\n",
    "tokenized_text=word_tokenize(Text)\n",
    "tagged_sent=nltk.pos_tag(tokenized_text)\n",
    "chunk_sent=nltk.ne_chunk(tagged_sent)\n",
    "named_entities=[]\n",
    "for tagged_tree in chunk_sent:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        #print(tagged_tree)\n",
    "        entity_name = ' '.join(c[0] for c in tagged_tree.leaves())\n",
    "        entity_type = tagged_tree.label()\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "\n",
    "print(named_entities)\n",
    "locations=[]\n",
    "for loc in named_entities:\n",
    "    if loc[1]=='GPE':\n",
    "        locations.append(loc[0])\n",
    "###Places Mentioned in the text\n",
    "print(set(locations))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Question No: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.20888888888888887, subjectivity=0.33425925925925926)\n"
     ]
    }
   ],
   "source": [
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Question No: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bag', 6), ('given', 3), ('gift', 3), ('member', 2), ('public', 2)]\n",
      "[(0, '0.061*\"bag\" + 0.032*\"given\" + 0.032*\"gift\"'), (1, '0.015*\"item\" + 0.015*\"buy\" + 0.015*\"among\"'), (2, '0.015*\"offer\" + 0.015*\"service\" + 0.015*\"item\"')]\n"
     ]
    }
   ],
   "source": [
    "most_comm=counts.most_common(5)\n",
    "print(most_comm)\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "words=[lmtzr.lemmatize(word.lower()) for word in word_tokenize(Text) if word.isalpha()]\n",
    "words_rem=[word for word in words if word not in custom]\n",
    "dictnry = Dictionary([words_rem])\n",
    "corp = [dictnry.doc2bow(article) for article in [words_rem]]\n",
    "import gensim\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(corp, num_topics=3, id2word = dictnry, passes=50)\n",
    "####Topic\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Question No: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "training = [\n",
    "('Tom Holland is a terrible spiderman.','pos'),\n",
    "('a terrible Javert (Russell Crowe) ruined Les Miserables for me...','pos'),\n",
    "('The Dark Knight Rises is the greatest superhero movie ever!','neg'),\n",
    "('Fantastic Four should have never been made.','pos'),\n",
    "('Wes Anderson is my favorite director!','neg'),\n",
    "('Captain America 2 is pretty awesome.','neg'),\n",
    "('Let\\s pretend \"Batman and Robin\" never happened..','pos'),\n",
    "]\n",
    "testing = [\n",
    "('Superman was never an interesting character.','pos'),\n",
    "('Fantastic Mr Fox is an awesome film!','neg'),\n",
    "('Dragonball Evolution is simply terrible!!','pos')]\n",
    "\n",
    "from textblob import classifiers\n",
    "classifier = classifiers.NaiveBayesClassifier(training)\n",
    "\n",
    "blob=tb('the weather is terrible',classifier=classifier)\n",
    "print(blob.classify())\n",
    "print(classifier.accuracy(testing))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Question No: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Twitter CEO Jack Dorsey on Monday said the issue of fake news “is a multi-variable problem and there won't be one fix”. “Fake news as a category is way too big… We have to make sure that we are scoping this problem as entirely as possible. Misinformation also comes in the form of jokes. We need to identify misleading information and try to make sure we are not helping in spreading it,” Dorsey, who is on a week-long visit to the country, said at a town hall event at IIT Delhi. \n",
      "\n",
      "“This is a multi-variable problem and there won’t be one fix. We will have to stay ten steps ahead. There will be a lot of technologies to help us but there can't be a perfect solution to this.” \n",
      "\n",
      "This is Dorsey’s first visit to India. Earlier in the day, Dorsey had met Rahul Gandhi, who later tweeted that they had discussed ways to tackle fake news.\n",
      "\n",
      "The Twitter CEO is likely to meet IT Minister Ravi Shankar Prasad too. Dorsey’s comments on fake news come in the aftermath of Twitter and other social media platforms coming under the scrutiny of regulators for issues like foreign interference in the US elections, fake news and propaganda through bots, and online abuse. \n",
      "\n",
      "In his interaction with the IIT students, Dorsey also said that Twitter is considering introducing an edit feature, but added that they “have to do it the right way”.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####https://economictimes.indiatimes.com/tech/internet/no-perfect-solution-to-curb-fake-news-says-jack-dorsey/articleshow/66599297.cms\n",
    "##No perfect solution to curb fake news, says Jack Dorsey##\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from bs4 import BeautifulSoup\n",
    "http = urllib3.PoolManager()\n",
    "response = http.request('GET',\"https://economictimes.indiatimes.com/tech/internet/no-perfect-solution-to-curb-fake-news-says-jack-dorsey/articleshow/66599297.cms\")\n",
    "soup = BeautifulSoup(response.data, 'html.parser')\n",
    "text = \". \".join([p.text for p in soup.find_all('div', {'class':'Normal'})])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Question No: 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('analytics', 'vidhya'), ('vidhya', 'is'), ('is', 'a'), ('a', 'great'), ('great', 'source'), ('source', 'to'), ('to', 'learn'), ('learn', 'data'), ('data', 'science')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "text_gram='Analytics Vidhya is a great source to learn data science'\n",
    "bigram = list(ngrams((x.lower() for x in nltk.word_tokenize(text_gram) if x not in list(punctuation)) , 2)) \n",
    "print(bigram)\n",
    "len(bigram)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Correct Option is : 9"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Question No: 9"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer is : Option B\n",
    "K * Log(3) / T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Question No: 10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer is : Option C --> 12534\n",
    "Order is:\n",
    "Text Cleaning --> Text annotation --> Text to predictors --> Gradient descent --> Model tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
