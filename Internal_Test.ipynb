{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"The Indian state of Kerala has been devastated by severe floods. More than 350 people have died, while more than a million have been evacuated to over 4,000 relief camps. Tens of thousands remain stranded. The crisis is a timely reminder that climate change is expected to increase the frequency and magnitude of severe flooding across the world. Although no single flood can be linked directly to climate change, basic physics attests to the fact that a warmer world and atmosphere will hold more water, which will result in more intense and extreme rainfall. The monsoon season usually brings heavy rains but this year Kerala has seen 42% more rain than would be expected, with more than 2,300mm of rain across the region since the beginning of June, and over 700mm in August alone. These are similar levels seen during Hurricane Harvey, that hit Houston in August 2017, when more than 1,500mm of rain fell during one storm. Tropical cyclones and hurricanes, such as Harvey, are expected to increase in strength by up to 10% with a 2℃ rise in global temperature. Under climate change the probability of such extreme rainfall is also predicted to grow by up to sixfold towards the end of the century. The rivers and drainage systems of Kerala have been unable to cope with such large volumes of water and this has resulted in flash flooding. Much of that water would normally be slowed down by trees or other natural obstacles. Yet over the past 40 years Kerala has lost nearly half its forest cover, an area of 9,000 km², just under the size of Greater London, while the state’s urban areas keep growing. This means that less rainfall is being intercepted, and more water is rapidly running into overflowing streams and rivers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Indian state of Kerala has been devastated by severe floods.', 'More than 350 people have died, while more than a million have been evacuated to over 4,000 relief camps.', 'Tens of thousands remain stranded.', 'The crisis is a timely reminder that climate change is expected to increase the frequency and magnitude of severe flooding across the world.', 'Although no single flood can be linked directly to climate change, basic physics attests to the fact that a warmer world and atmosphere will hold more water, which will result in more intense and extreme rainfall.', 'The monsoon season usually brings heavy rains but this year Kerala has seen 42% more rain than would be expected, with more than 2,300mm of rain across the region since the beginning of June, and over 700mm in August alone.', 'These are similar levels seen during Hurricane Harvey, that hit Houston in August 2017, when more than 1,500mm of rain fell during one storm.', 'Tropical cyclones and hurricanes, such as Harvey, are expected to increase in strength by up to 10% with a 2℃ rise in global temperature.', 'Under climate change the probability of such extreme rainfall is also predicted to grow by up to sixfold towards the end of the century.', 'The rivers and drainage systems of Kerala have been unable to cope with such large volumes of water and this has resulted in flash flooding.', 'Much of that water would normally be slowed down by trees or other natural obstacles.', 'Yet over the past 40 years Kerala has lost nearly half its forest cover, an area of 9,000 km², just under the size of Greater London, while the state’s urban areas keep growing.', 'This means that less rainfall is being intercepted, and more water is rapidly running into overflowing streams and rivers.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "psm = PorterStemmer()\n",
    "st = LancasterStemmer()\n",
    "lmtzr=WordNetLemmatizer()\n",
    "orignal_sentences = sent_tokenize(Text)\n",
    "print(orignal_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['indian', 'state', 'kerala', 'devast', 'sever', 'flood', '.', '350', 'peopl', 'die', ',', 'million', 'evacu', '4,000', 'relief', 'camp', '.', 'ten', 'thousand', 'remain', 'strand', '.', 'crisi', 'time', 'remind', 'climat', 'chang', 'expect', 'increas', 'frequenc', 'magnitud', 'sever', 'flood', 'across', 'world', '.', 'although', 'singl', 'flood', 'link', 'directli', 'climat', 'chang', ',', 'basic', 'physic', 'attest', 'fact', 'warmer', 'world', 'atmospher', 'hold', 'water', ',', 'result', 'intens', 'extrem', 'rainfal', '.', 'monsoon', 'season', 'usual', 'bring', 'heavi', 'rain', 'year', 'kerala', 'seen', '42', '%', 'rain', 'would', 'expect', ',', '2,300mm', 'rain', 'across', 'region', 'sinc', 'begin', 'june', ',', '700mm', 'august', 'alon', '.', 'similar', 'level', 'seen', 'hurrican', 'harvey', ',', 'hit', 'houston', 'august', '2017', ',', '1,500mm', 'rain', 'fell', 'one', 'storm', '.', 'tropic', 'cyclon', 'hurrican', ',', 'harvey', ',', 'expect', 'increas', 'strength', '10', '%', '2℃', 'rise', 'global', 'temperatur', '.', 'climat', 'chang', 'probabl', 'extrem', 'rainfal', 'also', 'predict', 'grow', 'sixfold', 'toward', 'end', 'centuri', '.', 'river', 'drainag', 'system', 'kerala', 'unabl', 'cope', 'larg', 'volum', 'water', 'result', 'flash', 'flood', '.', 'much', 'water', 'would', 'normal', 'slow', 'tree', 'natur', 'obstacl', '.', 'yet', 'past', '40', 'year', 'kerala', 'lost', 'nearli', 'half', 'forest', 'cover', ',', 'area', '9,000', 'km²', ',', 'size', 'greater', 'london', ',', 'state', '’', 'urban', 'area', 'keep', 'grow', '.', 'mean', 'less', 'rainfal', 'intercept', ',', 'water', 'rapidli', 'run', 'overflow', 'stream', 'river', '.']\n"
     ]
    }
   ],
   "source": [
    "paras_stemmedSansStopWords = [psm.stem(word.lower()) for word in word_tokenize(Text) if word.lower() not in stopwords.words('english')]\n",
    "print(paras_stemmedSansStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_para = \" \".join([word for word in paras_stemmedSansStopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indian state kerala devast sever flood . 350 peopl die , million evacu 4,000 relief camp . ten thousand remain strand . crisi time remind climat chang expect increas frequenc magnitud sever flood across world . although singl flood link directli climat chang , basic physic attest fact warmer world atmospher hold water , result intens extrem rainfal . monsoon season usual bring heavi rain year kerala seen 42 % rain would expect , 2,300mm rain across region sinc begin june , 700mm august alon . similar level seen hurrican harvey , hit houston august 2017 , 1,500mm rain fell one storm . tropic cyclon hurrican , harvey , expect increas strength 10 % 2℃ rise global temperatur . climat chang probabl extrem rainfal also predict grow sixfold toward end centuri . river drainag system kerala unabl cope larg volum water result flash flood . much water would normal slow tree natur obstacl . yet past 40 year kerala lost nearli half forest cover , area 9,000 km² , size greater london , state ’ urban area keep grow . mean less rainfal intercept , water rapidli run overflow stream river .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['indian', 'state', 'kerala', 'devast', 'sever', 'flood', '350', 'peopl', 'die', 'million', 'evacu', '4,000', 'relief', 'camp', 'ten', 'thousand', 'remain', 'strand', 'crisi', 'time', 'remind', 'climat', 'chang', 'expect', 'increas', 'frequenc', 'magnitud', 'sever', 'flood', 'across', 'world', 'although', 'singl', 'flood', 'link', 'directli', 'climat', 'chang', 'basic', 'physic', 'attest', 'fact', 'warmer', 'world', 'atmospher', 'hold', 'water', 'result', 'intens', 'extrem', 'rainfal', 'monsoon', 'season', 'usual', 'bring', 'heavi', 'rain', 'year', 'kerala', 'seen', '42', 'rain', 'would', 'expect', '2,300mm', 'rain', 'across', 'region', 'sinc', 'begin', 'june', '700mm', 'august', 'alon', 'similar', 'level', 'seen', 'hurrican', 'harvey', 'hit', 'houston', 'august', '2017', '1,500mm', 'rain', 'fell', 'one', 'storm', 'tropic', 'cyclon', 'hurrican', 'harvey', 'expect', 'increas', 'strength', '10', '2℃', 'rise', 'global', 'temperatur', 'climat', 'chang', 'probabl', 'extrem', 'rainfal', 'also', 'predict', 'grow', 'sixfold', 'toward', 'end', 'centuri', 'river', 'drainag', 'system', 'kerala', 'unabl', 'cope', 'larg', 'volum', 'water', 'result', 'flash', 'flood', 'much', 'water', 'would', 'normal', 'slow', 'tree', 'natur', 'obstacl', 'yet', 'past', '40', 'year', 'kerala', 'lost', 'nearli', 'half', 'forest', 'cover', 'area', '9,000', 'km²', 'size', 'greater', 'london', 'state', 'urban', 'area', 'keep', 'grow', 'mean', 'less', 'rainfal', 'intercept', 'water', 'rapidli', 'run', 'overflow', 'stream', 'river']\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "custom = set(stopwords.words('english')+list(punctuation) + [\"'\",'\"',\"“\",'’'])\n",
    "stemmed_words = [word for word in paras_stemmedSansStopWords if word not in custom]\n",
    "print(stemmed_words)\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'kerala': 4, 'flood': 4, 'water': 4, 'rain': 4, 'climat': 3, 'chang': 3, 'expect': 3, 'rainfal': 3, 'state': 2, 'sever': 2, 'increas': 2, 'across': 2, 'world': 2, 'result': 2, 'extrem': 2, 'year': 2, 'seen': 2, 'would': 2, 'august': 2, 'hurrican': 2, 'harvey': 2, 'grow': 2, 'river': 2, 'area': 2, 'indian': 1, 'devast': 1, '350': 1, 'peopl': 1, 'die': 1, 'million': 1, 'evacu': 1, '4,000': 1, 'relief': 1, 'camp': 1, 'ten': 1, 'thousand': 1, 'remain': 1, 'strand': 1, 'crisi': 1, 'time': 1, 'remind': 1, 'frequenc': 1, 'magnitud': 1, 'although': 1, 'singl': 1, 'link': 1, 'directli': 1, 'basic': 1, 'physic': 1, 'attest': 1, 'fact': 1, 'warmer': 1, 'atmospher': 1, 'hold': 1, 'intens': 1, 'monsoon': 1, 'season': 1, 'usual': 1, 'bring': 1, 'heavi': 1, '42': 1, '2,300mm': 1, 'region': 1, 'sinc': 1, 'begin': 1, 'june': 1, '700mm': 1, 'alon': 1, 'similar': 1, 'level': 1, 'hit': 1, 'houston': 1, '2017': 1, '1,500mm': 1, 'fell': 1, 'one': 1, 'storm': 1, 'tropic': 1, 'cyclon': 1, 'strength': 1, '10': 1, '2℃': 1, 'rise': 1, 'global': 1, 'temperatur': 1, 'probabl': 1, 'also': 1, 'predict': 1, 'sixfold': 1, 'toward': 1, 'end': 1, 'centuri': 1, 'drainag': 1, 'system': 1, 'unabl': 1, 'cope': 1, 'larg': 1, 'volum': 1, 'flash': 1, 'much': 1, 'normal': 1, 'slow': 1, 'tree': 1, 'natur': 1, 'obstacl': 1, 'yet': 1, 'past': 1, '40': 1, 'lost': 1, 'nearli': 1, 'half': 1, 'forest': 1, 'cover': 1, '9,000': 1, 'km²': 1, 'size': 1, 'greater': 1, 'london': 1, 'urban': 1, 'keep': 1, 'mean': 1, 'less': 1, 'intercept': 1, 'rapidli': 1, 'run': 1, 'overflow': 1, 'stream': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(stemmed_words)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(sent):\n",
    "    words = word_tokenize(sent)\n",
    "    score = [counts[x] for x in words]\n",
    "    return sum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 8, 4, 26, 35, 42, 21, 17, 20, 23, 12, 29, 16]\n"
     ]
    }
   ],
   "source": [
    "scores = [score(sent) for sent in sent_tokenize(stemmed_para)]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although no single flood can be linked directly to climate change, basic physics attests to the fact that a warmer world and atmosphere will hold more water, which will result in more intense and extreme rainfall.\n",
      "The monsoon season usually brings heavy rains but this year Kerala has seen 42% more rain than would be expected, with more than 2,300mm of rain across the region since the beginning of June, and over 700mm in August alone.\n",
      "Yet over the past 40 years Kerala has lost nearly half its forest cover, an area of 9,000 km², just under the size of Greater London, while the state’s urban areas keep growing.\n"
     ]
    }
   ],
   "source": [
    "threshold = sorted(scores)[-4]\n",
    "for i,x in enumerate(scores):\n",
    "    if x > threshold:\n",
    "        print(orignal_sentences[i])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kerala', 'NNP')\n",
      "('Kerala', 'NNP')\n",
      "('June', 'NNP')\n",
      "('August', 'NNP')\n",
      "('Hurricane', 'NNP')\n",
      "('Harvey', 'NNP')\n",
      "('Houston', 'NNP')\n",
      "('August', 'NNP')\n",
      "('Harvey', 'NNP')\n",
      "('Kerala', 'NNP')\n",
      "('Kerala', 'NNP')\n",
      "('Greater', 'NNP')\n",
      "('London', 'NNP')\n",
      "('’', 'NNP')\n",
      "[('Indian', 'GPE'), ('Kerala', 'GPE'), ('Kerala', 'PERSON'), ('Hurricane Harvey', 'PERSON'), ('Houston', 'PERSON'), ('Harvey', 'PERSON'), ('Kerala', 'GPE'), ('Kerala', 'PERSON'), ('Greater London', 'ORGANIZATION')]\n",
      "{'Kerala', 'Indian'}\n"
     ]
    }
   ],
   "source": [
    "###Question#2\n",
    "import nltk\n",
    "from textblob import TextBlob as tb\n",
    "blob=tb(Text)\n",
    "#print(blob)\n",
    "a=blob.tags\n",
    "#print(blob.tags)\n",
    "for i,x in enumerate(a):\n",
    "    if a[i][1]=='NNP':\n",
    "        print(a[i])\n",
    "tokenized_text=word_tokenize(Text)\n",
    "tagged_sent=nltk.pos_tag(tokenized_text)\n",
    "chunk_sent=nltk.ne_chunk(tagged_sent)\n",
    "named_entities=[]\n",
    "for tagged_tree in chunk_sent:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        #print(tagged_tree)\n",
    "        entity_name = ' '.join(c[0] for c in tagged_tree.leaves())\n",
    "        entity_type = tagged_tree.label()\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "\n",
    "print(named_entities)\n",
    "locations=[]\n",
    "for loc in named_entities:\n",
    "    if loc[1]=='GPE':\n",
    "        locations.append(loc[0])\n",
    "###Places Mentioned in the text\n",
    "print(set(locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.08340840840840842, subjectivity=0.41662376662376666)\n"
     ]
    }
   ],
   "source": [
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kerala', 4), ('flood', 4), ('water', 4), ('rain', 4), ('climat', 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aishwarya\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.008*\"end\" + 0.008*\"camp\" + 0.008*\"natural\"'), (1, '0.022*\"rain\" + 0.022*\"ha\" + 0.022*\"kerala\"'), (2, '0.008*\"fact\" + 0.008*\"season\" + 0.008*\"unable\"')]\n"
     ]
    }
   ],
   "source": [
    "most_comm=counts.most_common(5)\n",
    "print(most_comm)\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "words=[lmtzr.lemmatize(word.lower()) for word in word_tokenize(Text) if word.isalpha()]\n",
    "words_rem=[word for word in words if word not in custom]\n",
    "dictnry = Dictionary([words_rem])\n",
    "corp = [dictnry.doc2bow(article) for article in [words_rem]]\n",
    "import gensim\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(corp, num_topics=3, id2word = dictnry, passes=50)\n",
    "####Topic\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "training = [\n",
    "('Tom Holland is a terrible spiderman.','pos'),\n",
    "('a terrible Javert (Russell Crowe) ruined Les Miserables for me...','pos'),\n",
    "('The Dark Knight Rises is the greatest superhero movie ever!','neg'),\n",
    "('Fantastic Four should have never been made.','pos'),\n",
    "('Wes Anderson is my favorite director!','neg'),\n",
    "('Captain America 2 is pretty awesome.','neg'),\n",
    "('Let\\s pretend \"Batman and Robin\" never happened..','pos'),\n",
    "]\n",
    "testing = [\n",
    "('Superman was never an interesting character.','pos'),\n",
    "('Fantastic Mr Fox is an awesome film!','neg'),\n",
    "('Dragonball Evolution is simply terrible!!','pos')]\n",
    "\n",
    "from textblob import classifiers\n",
    "classifier = classifiers.NaiveBayesClassifier(training)\n",
    "blob=tb('the weather is terrible',classifier=classifier)\n",
    "print(blob.classify())\n",
    "print(classifier.accuracy(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Question#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUMBAI: Colgate-Palmolive, the company behind the eponymous toothpaste, said innovation in India to compete within ayurvedic segment has helped launch similar brands in global markets too.\n",
      "\n",
      "\"We have stepped into now changing the shape of our portfolio as part of innovation. And that may be the transfer of a global brand to a new geography or a new retail channel, as we have described or indeed the development of a local brand, either in response to a competitor, like Vedshakti in India now transferred under Colgate Naturals Panjaved (in Thailand),\" Ian Cook, global chief executive of the US multinational, said in an investors’ call.\n",
      "\n",
      "Vedshakti, launched nearly two years ago in India, was aimed squarely at the Baba Ramdev-led Patanjali, which has challenged the multinational's dominance of the segment with its Dant Kanti toothpaste. Like many large consumer goods companies including Unilever, Colgate had struggled with sluggish volume growth and shifting consumer taste towards natural or herbal brands.\n",
      "\n",
      "Colgate had been steadily losing its market share in recent years with consumer taste increasingly shifting towards natural or herbal brands. In terms of volume or actual products that consumers put in the shopping basket, Colgate's share has consistently fallen from its peak of 57.8% in 2014-15 to 53.4% in 2017-18, according to Colgate India annual reports.\n",
      "\n",
      "However, Colgate managed to arrest its share decline last quarter. It said India's share performance is beginning to improve behind the innovation and the advertising support.\n",
      "\n",
      "\"We continue to deliver volume growth with positive pricing despite high levels of competitive activity. We are encouraged with the market share performance of our Swarna and Cibaca Vedshakti toothpaste lines,\" John Faucher SVP - Investor Relations at Colgate-Palmolive Company told investors. \"As we see further distribution gains and continue to invest in advertising, we expect our share in the Ayurvedic segment of the toothpaste category to continue to grow.\"\n",
      "\n",
      " With 25% of India’s toothpaste market being herbal now, analysts feel Colgate is getting on a strong footing with a changing toothpaste portfolio. Analysts said it remained confident of Colgate's ability to counter rivals. \"Colgate’s natural portfolio, Colgate Swarna Vedshakti, has been doing well across markets that it has been launched in, with market share crossing 3% in select geographies. We maintain that Colgate’s market share loss has largely bottomed out, but improvement is likely to be gradual,\" said Abneesh Roy, senior vice-president, institutional equities, Edelweiss Securities.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####https://economictimes.indiatimes.com/industry/cons-products/fmcg/colgate-to-piggyback-on-ayurveda-for-global-innovation/articleshow/66429696.cms \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from bs4 import BeautifulSoup\n",
    "http = urllib3.PoolManager()\n",
    "response = http.request('GET',\"https://economictimes.indiatimes.com/industry/cons-products/fmcg/colgate-to-piggyback-on-ayurveda-for-global-innovation/articleshow/66429696.cms\")\n",
    "soup = BeautifulSoup(response.data, 'html.parser')\n",
    "text = \". \".join([p.text for p in soup.find_all('div', {'class':'Normal'})])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Question#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batman', 'is'), ('is', 'a'), ('a', 'fictional'), ('fictional', 'superhero'), ('superhero', 'appearing'), ('appearing', 'in'), ('in', 'american'), ('american', 'comic'), ('comic', 'books'), ('books', 'published'), ('published', 'by'), ('by', 'dc'), ('dc', 'comics')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "text_gram='Batman is a fictional superhero appearing in American comic books published by DC Comics.'\n",
    "bigram = list(ngrams((x.lower() for x in nltk.word_tokenize(text_gram) if x not in list(punctuation)) , 2)) \n",
    "print(bigram)\n",
    "len(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Question#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New', 'Delhi', 'Analytics', 'Vidhya', 'Delhi', 'Hackathon']\n",
      "['am', 'planning', 'visit', 'attend']\n",
      "Delhi\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "text_2=\"I am planning to visit New Delhi to attend Analytics Vidhya Delhi Hackathon\"\n",
    "words_2=[word for word in word_tokenize(text_2) if word not in custom]\n",
    "count_words2=Counter(words_2)\n",
    "noun=[token for token, pos in pos_tag(word_tokenize(text_2)) if pos.startswith('N')]      \n",
    "print(noun)\n",
    "\n",
    "verb=[token for token, pos in pos_tag(word_tokenize(text_2)) if pos.startswith('V')]      \n",
    "print(verb)\n",
    "\n",
    "for w in count_words2:\n",
    "    if count_words2[w]>1:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Question#9 MCQ\n",
    "Answer----->>>>>> Option B- K * Log(3) / T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Question#10 MCQ\n",
    "Answer----->>>>>> 12534\n",
    "Text cleaning >>>>>>> Text annotation>>>>>>>> Text to predictors>>>>>>> Gradient descent>>>>> Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
